{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELING\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Select modeling techniques: Determine which algorithms to try (e.g. regression, neural\n",
    "# net).\n",
    "# \n",
    "# Generate test design: Pending your modeling approach, you might need to split the data\n",
    "# into training, test, and validation sets.\n",
    "# \n",
    "# Build model: As glamorous as this might sound, this might just be executing a few\n",
    "# lines of code like “reg = LinearRegression().fit(X, y)”.\n",
    "# \n",
    "# Assess model: Generally, multiple models are competing against each other, and the\n",
    "# data scientist needs to interpret the model results based on domain knowledge, the\n",
    "# pre-defined success criteria, and the test design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCLUSIONS\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# - Big cities are more prone to accidents (ofc, more people, more cars, more accidents)\n",
    "# - Some clusters could be merged, this is specially true for clusters in cities\n",
    "# - Liked the idea of removing noise (-1) from the data, but not sure if it's actually\n",
    "# noise (check original data). IMO these sparse points are not relevant for the analysis\n",
    "# \n",
    "# - Filter our some results, so there are less to analyze\n",
    "# - Minimum number of clusters: 15\n",
    "# - Maximum number of clusters: sqrt(n_samples) = 140\n",
    "# \n",
    "# - We want accidents close to each other: small eps\n",
    "# - We want clusters with many accidents: high min_samples\n",
    "# \n",
    "# - If min_samples is too high, too few cluster (only around big cities)\n",
    "# - If min_samples is too low, too much clusters\n",
    "# - If eps is too big (0.75), too big clusters + too few n_noise\n",
    "# - If eps is too small, too small clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "YEAR = \"2021\"\n",
    "\n",
    "shutil.rmtree(f\"../output/d-modeling\")\n",
    "os.makedirs(f\"../output/d-modeling\", exist_ok=True)\n",
    "os.makedirs(f\"../data/d-modeling\", exist_ok=True)\n",
    "\n",
    "file = f\"../data/c-data-preparation/{YEAR}-raw.csv\"\n",
    "raw = pd.read_csv(file)\n",
    "\n",
    "file = f\"../data/c-data-preparation/{YEAR}-preprocessed.csv\"\n",
    "preprocessed = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['latitude', 'longitude']\n",
      "2: ['latitude', 'longitude', 'br', 'km', 'municipio']\n"
     ]
    }
   ],
   "source": [
    "# Group features\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# geograficas: latitude, longitude, uf, br, km, municipio\n",
    "# caracteristicas: condicao_metereologica, causa_acidente, tipo_acidente, classificacao_acidente\n",
    "# estatisticas: pessoas, mortos, feridos_leves, feridos_graves, ilesos, ignorados, feridos, veiculos\n",
    "# temporais: ano, mes, dia, hora, minuto, dia_semana, fase_dia\n",
    "# pista: sentido_via, tipo_pista, tracado_via, uso_solo\n",
    "# prf: regional, delegacia, uop\n",
    "\n",
    "# espaciais = [\"latitude\", \"longitude\", \"br\", \"km\", \"municipio\"]\n",
    "espaciais = [\"latitude\", \"longitude\", \"br\", \"km\"]\n",
    "\n",
    "# Generate and chain all combinations using a list comprehension\n",
    "combinations = [itertools.combinations(espaciais, r) for r in range(1, len(espaciais) + 1)]\n",
    "combinations = list(itertools.chain.from_iterable(combinations))\n",
    "\n",
    "# Filter out combinations that do not have both \"latitude\" and \"longitude\"\n",
    "combinations = [item for item in combinations if \"latitude\" in item and \"longitude\" in item]\n",
    "\n",
    "# Convert sets to lists\n",
    "combinations = [list(item) for item in combinations]\n",
    "\n",
    "combinations = [\n",
    "    [\"latitude\", \"longitude\"],\n",
    "    [\"latitude\", \"longitude\", \"municipio\"],\n",
    "    [\"latitude\", \"longitude\", \"br\", \"km\", \"municipio\"],\n",
    "]\n",
    "\n",
    "for i, item in enumerate(combinations):\n",
    "    print(f\"{i + 1}: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting clusters on a map\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def plot(df, labels, dst, title):\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    color_scale = [\"hsl(\" + str(h) + \",50%\" + \",50%)\" for h in np.linspace(0, 360, num_clusters)]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for cluster_label, color in zip(np.unique(labels), color_scale):\n",
    "        # Skip noise\n",
    "        if cluster_label == -1:\n",
    "            continue\n",
    "\n",
    "        cluster_mask = labels == cluster_label\n",
    "        cluster_data = df[cluster_mask]\n",
    "\n",
    "        scatter = go.Scattermapbox(\n",
    "            lat=cluster_data.latitude,\n",
    "            lon=cluster_data.longitude,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=7.5, color=color),\n",
    "            # text=f\"({cluster_label}) {cluster_data.causa_acidente}\",\n",
    "            text=f\"{cluster_label}\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(scatter)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        title_x=0.95,\n",
    "        title_y=0.1,\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox=dict(center=dict(lat=-28, lon=-52), zoom=5.5),\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n",
    "    )\n",
    "\n",
    "    # fig.show()\n",
    "    fig.write_html(f\"{dst}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: euclidean, eps: 0.1, min_samples: 100\n",
      "---------------------------------------------\n",
      "Columns: ['latitude', 'longitude']\n",
      "Number of clusters is outside allowed range\n",
      "Skipping, condition not met: 15 <= 1 <= 140\n",
      "\n",
      "metric: euclidean, eps: 0.1, min_samples: 100\n",
      "---------------------------------------------\n",
      "Columns: ['latitude', 'longitude', 'br', 'km', 'municipio']\n",
      "Estimated number of clusters: 48\n",
      "Estimated number of noise points: 7645\n",
      "Silhouette coefficient: 0.244\n",
      "\n",
      "metric: euclidean, eps: 0.1, min_samples: 250\n",
      "---------------------------------------------\n",
      "Columns: ['latitude', 'longitude']\n",
      "Number of clusters is outside allowed range\n",
      "Skipping, condition not met: 15 <= 1 <= 140\n",
      "\n",
      "metric: euclidean, eps: 0.1, min_samples: 250\n",
      "---------------------------------------------\n",
      "Columns: ['latitude', 'longitude', 'br', 'km', 'municipio']\n",
      "Number of clusters is outside allowed range\n",
      "Skipping, condition not met: 15 <= 13 <= 140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build models\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "METRIC = \"euclidean\"\n",
    "EPS = [0.1]\n",
    "MIN_SAMPLES = [100, 250]\n",
    "COLUMNS = combinations\n",
    "\n",
    "length = len(preprocessed)\n",
    "minimum = 15\n",
    "maximum = int(np.sqrt(length))\n",
    "\n",
    "hyperparameters = itertools.product(EPS, MIN_SAMPLES, COLUMNS)\n",
    "for i, (eps, min_samples, columns) in enumerate(hyperparameters):\n",
    "    header = f\"metric: {METRIC}, eps: {eps}, min_samples: {min_samples}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(f\"Columns: {columns}\")\n",
    "\n",
    "    X_train = preprocessed[columns].values\n",
    "\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples, metric=METRIC)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    labels = model.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    if not (minimum <= n_clusters <= maximum):\n",
    "        print(f\"Number of clusters is outside allowed range\")\n",
    "        print(f\"Skipping, condition not met: {minimum} <= {n_clusters} <= {maximum}\\n\")\n",
    "        continue\n",
    "\n",
    "    score = metrics.silhouette_score(X_train, labels)\n",
    "\n",
    "    dst = f\"../output/d-modeling/{METRIC}-{eps:.2f}-{min_samples}-{i}\"\n",
    "    title = (\n",
    "        f\"n_clusters: {n_clusters}, n_noise: {n_noise}, score: {score:.3f}<br>\"\n",
    "        f\"columns: {columns}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "    print(f\"Estimated number of noise points: {n_noise}\")\n",
    "    print(f\"Silhouette coefficient: {score:.3f}\\n\")\n",
    "\n",
    "    plot(raw, labels, dst, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model: trechos\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# - adding br, km and municipio helps on diversity of clusters\n",
    "# - eps around 0.1 looks a good choice\n",
    "# - min_samples around 50 looks a good choice\n",
    "\n",
    "EPS = 0.1\n",
    "MIN_SAMPLES = 50\n",
    "COLUMNS = [\"latitude\", \"longitude\", \"br\", \"km\", \"municipio\"]\n",
    "METRIC = \"euclidean\"\n",
    "\n",
    "length = len(preprocessed)\n",
    "\n",
    "header = f\"metric: {METRIC}, eps: {EPS}, min_samples: {MIN_SAMPLES}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "X_train = preprocessed[COLUMNS].values\n",
    "\n",
    "model = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric=METRIC)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present\n",
    "labels = model.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "score = metrics.silhouette_score(X_train, labels)\n",
    "\n",
    "title = f\"n_clusters: {n_clusters}, n_noise: {n_noise}, score: {score:.3f}\"\n",
    "\n",
    "print(f\"Columns: {COLUMNS}\")\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Estimated number of noise points: {n_noise}\")\n",
    "print(f\"Silhouette coefficient: {score:.3f}\\n\")\n",
    "\n",
    "# Save dataset with cluster column to csv\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "raw[\"cluster\"] = model.labels_\n",
    "raw.to_csv(f\"../data/d-modeling/{YEAR}-trechos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model: regioes\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# NOTE: avoid duplicated code here\n",
    "\n",
    "EPS = None\n",
    "MIN_SAMPLES = None\n",
    "COLUMNS = None\n",
    "METRIC = \"euclidean\"\n",
    "\n",
    "length = len(preprocessed)\n",
    "\n",
    "header = f\"metric: {METRIC}, eps: {EPS}, min_samples: {MIN_SAMPLES}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "X_train = preprocessed[COLUMNS].values\n",
    "\n",
    "model = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric=METRIC)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present\n",
    "labels = model.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "score = metrics.silhouette_score(X_train, labels)\n",
    "\n",
    "title = f\"n_clusters: {n_clusters}, n_noise: {n_noise}, score: {score:.3f}\"\n",
    "\n",
    "print(f\"Columns: {columns}\")\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Estimated number of noise points: {n_noise}\")\n",
    "print(f\"Silhouette coefficient: {score:.3f}\\n\")\n",
    "\n",
    "# Save dataset with cluster column to csv\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "raw[\"cluster\"] = model.labels_\n",
    "raw.to_csv(f\"../data/d-modeling/{YEAR}-regioes.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
